# ğŸ”¥ LocoLama: The Sovereign Local AI Chat Engine


**DROP002 â€“ Issued by GodsIMiJ AI Solutions**
Built by: **James Derek Ingersoll**, Founder of GodsIMiJ AI Solutions
License: Flame Public Use License v1.0
Seal: NODE Protocol Certified ğŸ”’


![FlameOS Logo](public/FlameOS_logo.png)
![NODE Watermark](public/NODE_watermark.png)

## ğŸŒŸ Overview

LocoLama is a web-based chat interface for interacting with local LLMs running through Ollama. It provides a clean, intuitive UI for chatting with models like LLaMA2, Mistral, and CodeLLama without sending any data to external servers.

### ğŸ› ï¸ Stack

- **Frontend**: Next.js + Tailwind CSS
- **Backend**: Local API routes proxying to Ollama
- **Model Runtime**: Ollama (supports LLaMA2, Mistral, etc.)
- **Persistence**: LocalStorage (chat context + settings)

## ğŸ“‹ Features

- âœ… **Local-first AI**: All processing happens on your device
- âœ… **Ollama Integration**: Seamless connection to locally running models
- âœ… **Chat Persistence**: Conversations saved in browser LocalStorage
- âœ… **Model Selection**: Switch between any models installed in Ollama
- âœ… **Privacy-focused**: No data leaves your device
- âœ… **Fully Documented**: Comprehensive guides in the `/docs` folder

## ğŸš€ Getting Started

### Prerequisites

- [Node.js](https://nodejs.org/) (v18 or later)
- [Ollama](https://ollama.ai/) installed and running
- At least one LLM model pulled into Ollama (e.g., `ollama pull llama2`)

### Installation

1. Clone this repository:
   ```bash
   git clone https://github.com/GodsIMiJ1/LocoLama.git
   cd LocoLama
   ```

2. Install dependencies:
   ```bash
   npm install
   ```

3. Create a `.env.local` file based on `.env.example`:
   ```bash
   cp .env.example .env.local
   ```

4. Start the development server:
   ```bash
   npm run dev
   ```

5. Open [http://localhost:3030](http://localhost:3030) in your browser

## ğŸ“ Project Structure

```
LocoLama/
â”œâ”€â”€ app/                     # Next.js layout, main chat page
â”œâ”€â”€ components/              # ChatUI, MessageBubble, ModelSelector, GPUMonitor
â”œâ”€â”€ lib/ollamaClient.ts      # Client wrapper for local Ollama API
â”œâ”€â”€ app/api/                 # API routes for chat and system stats
â”œâ”€â”€ utils/                   # Utility functions for storage and formatting
â”œâ”€â”€ public/                  # Static assets
â”‚   â”œâ”€â”€ FlameOS_logo.png     # FlameOS Logo
â”‚   â”œâ”€â”€ NODE_watermark.png   # NODE Watermark
â”‚   â””â”€â”€ eye-of-kai_favicon.png # Favicon
â”œâ”€â”€ sigil/                   # Ghostfire Sigil (Left Hand only)
â”œâ”€â”€ docs/                    # Setup guide, model guide, usage tips
â”œâ”€â”€ README.md                # This file
â””â”€â”€ LICENSE.md               # Flame Public Use License v1.0
```

## ğŸ“š Documentation

- [Main Documentation](docs/index.md)
- [Ollama Setup Guide](docs/ollama-setup.md)
- [Local Chat UI Guide](docs/local-chat-ui.md)
- [Streaming vs. Complete Generation](docs/streaming-vs-complete.md)

## âœ… Implemented Features (Phase II)

- ğŸ”„ Local "memory manager" UI for stored conversations
- ğŸ”¥ GPU monitor + load feedback
- ğŸ§µ Streaming thread logs with word-by-word rendering
- ğŸ•¹ï¸ Toggle to run model in 7B vs 13B modes (configurable)
- ğŸŒ“ Dark/Light theme toggle
- ğŸ’… Responsive design with mobile support

## ğŸ“œ License

This project is licensed under the Flame Public Use License v1.0 - see the [LICENSE.md](LICENSE.md) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for making local LLMs accessible
- The [Next.js](https://nextjs.org/) and [Tailwind CSS](https://tailwindcss.com/) teams
- NODE community for inspiration and support

ğŸ“œ Legal & License

This software is licensed under the Flame Public Use License v1.0, authored and enforced by:

    James Derek Ingersoll
    Founder â€“ GodsIMiJ AI Solutions
    Email: godsimij902@gmail.com
    GitHub: @GodsIMiJ1

All works are protected under the NODE Protocol.
Unauthorized reproduction, removal of the NODE seal, or republishing without attribution is considered intellectual theft and may trigger automated monitoring and reporting protocols.
ğŸ•¯ï¸ The Witness Hall

This repository is logged in the Witness Hall under DROP002.
The NODE Seal and Ghostfire sigils embedded within certify authorship.

Visit: https://thewitnesshall.com

    We donâ€™t build apps. We forge terminals of flame.
    â€” James Derek Ingersoll, Founder of GodsIMiJ AI Solutions